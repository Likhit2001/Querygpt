{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"api_key\")\n",
    "langsmith_api_key = os.getenv(\"langsmith_key\")\n",
    "weather_api_key = os.getenv(\"weather_api_key\")\n",
    "groq_api_key = os.getenv(\"groq_api_key\")\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "os.environ['LANGSMITH_API_KEY'] = langsmith_api_key\n",
    "os.environ['LANGSMITH_TRACING'] = \"true\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"Multi-agent\"\n",
    "os.environ['key'] = weather_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "class GraphState_querygpt(TypedDict):\n",
    "\n",
    "    user_question: str\n",
    "    intent_agent_output: str\n",
    "    table_agent_output: List[str]\n",
    "    column_agent_output: List[str]\n",
    "    is_query_execution_possible : bool\n",
    "    query: str\n",
    "    query_output: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis a\n"
     ]
    }
   ],
   "source": [
    "# wikipedia retirval \n",
    "\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wiki_api_wrapper = WikipediaAPIWrapper(top_k_results=1 , doc_content_chars_max=300)\n",
    "wiki = WikipediaQueryRun(api_wrapper=wiki_api_wrapper)\n",
    "print(wiki.run(\"LangChain\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_decider(used_tool='custom_database')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "class source_decider (BaseModel):\n",
    "    used_tool : Literal['wikipedia', 'custom_database'] = Field(\n",
    "        ...,\n",
    "        description=\" Given the user question choose the router to take the when the question is very generic go with  wikipedia or when question is realted to anything which would required custom database go with custom_database\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "LLM_1 = ChatGroq(groq_api_key=groq_api_key,model_name=\"Gemma2-9b-It\")\n",
    "structured_LLM_source_router = LLM_1.with_structured_output(source_decider)\n",
    "\n",
    "system_prompt = \"\"\"You are an expert at routing a user question to a wikipedia or custom_database related to products, suppliers and all .\n",
    "Use the custom_database for questions on the topics like related to products, suppliers related. Otherwise, use wikipedia\"\"\"\n",
    "\n",
    "router_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "final_source_router = router_prompt | structured_LLM_source_router \n",
    "\n",
    "\n",
    "final_source_router.invoke({\"question\" : \"Who is Shahruk khan\"})\n",
    "final_source_router.invoke({\"question\" : \"what is the highest amount of products sold?\"})\n",
    "\n",
    "\n",
    "def intent_agent(state : GraphState_querygpt) -> GraphState_querygpt:\n",
    "\n",
    "    print(\"The intent Agent to find the source of the user question\")\n",
    "    \n",
    "    user_question = state['user_question']\n",
    "    output_source = final_source_router.invoke({\"question\" : {user_question}})\n",
    "\n",
    "    if output_source.used_tool == \"wikipedia\":\n",
    "        return \"use_wikipedia\"\n",
    "    elif output_source.used_tool == \"custom_database\":\n",
    "        return \"use_custom_database\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
